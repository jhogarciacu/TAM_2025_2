{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/edwingarcia48/nfl-big-data-bowl-2026-analysis-prediction-3ea6f5?scriptVersionId=268125345\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T15:00:02.889301Z","iopub.execute_input":"2025-10-06T15:00:02.88965Z","iopub.status.idle":"2025-10-06T15:00:02.926571Z","shell.execute_reply.started":"2025-10-06T15:00:02.889623Z","shell.execute_reply":"2025-10-06T15:00:02.925586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pickle\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# GPU CONFIGURATION & OPTIMIZATION\n# ============================================================================\n\ndef setup_gpu():\n    \"\"\"Configure GPU for optimal performance\"\"\"\n    print(\"=\"*80)\n    print(\"GPU CONFIGURATION\")\n    print(\"=\"*80)\n    \n    # Check available GPUs\n    gpus = tf.config.list_physical_devices('GPU')\n    print(f\"\\nüñ•Ô∏è  Available GPUs: {len(gpus)}\")\n    \n    if gpus:\n        try:\n            # Enable memory growth (don't allocate all GPU memory at once)\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n                print(f\"   ‚úì GPU: {gpu.name} - Memory growth enabled\")\n            \n            # Set GPU memory limit (optional - useful if sharing GPU)\n            # tf.config.set_logical_device_configuration(\n            #     gpus[0],\n            #     [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  # 4GB\n            # )\n            \n            # Use mixed precision for faster training\n            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n            tf.keras.mixed_precision.set_global_policy(policy)\n            print(f\"   ‚úì Mixed precision enabled: {policy.name}\")\n            \n            logical_gpus = tf.config.list_logical_devices('GPU')\n            print(f\"   ‚úì Logical GPUs: {len(logical_gpus)}\")\n            \n        except RuntimeError as e:\n            print(f\"   ‚ö†Ô∏è  GPU configuration error: {e}\")\n    else:\n        print(\"   ‚ö†Ô∏è  No GPU found - using CPU (training will be slower)\")\n    \n    # Set TensorFlow options for better performance\n    tf.config.optimizer.set_jit(True)  # XLA compilation\n    print(\"   ‚úì XLA (Accelerated Linear Algebra) enabled\")\n    \n    print(f\"\\nüìä TensorFlow version: {tf.__version__}\")\n    print(f\"üìä Keras version: {keras.__version__}\")\n    \n    return len(gpus) > 0\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nCONFIG = {\n    'sequence_length': 10,\n    'max_frames_to_predict': 15,\n    'batch_size': 256,  # Larger batch for GPU\n    'epochs': 100,\n    'learning_rate': 0.001,\n    'validation_split': 0.15,\n    'use_gpu': True,\n}\n\n# ============================================================================\n# EVALUATION METRICS\n# ============================================================================\n\ndef calculate_rmse(y_true, y_pred):\n    \"\"\"Calculate Root Mean Squared Error\"\"\"\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n    return rmse\n\ndef calculate_mae(y_true, y_pred):\n    \"\"\"Calculate Mean Absolute Error\"\"\"\n    mae = np.mean(np.abs(y_true - y_pred))\n    return mae\n\ndef calculate_euclidean_distance(y_true, y_pred):\n    \"\"\"Calculate Euclidean distance between predicted and actual positions\"\"\"\n    distances = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 + \n                       (y_true[:, 1] - y_pred[:, 1])**2)\n    return distances\n\ndef evaluate_predictions(y_true, y_pred, split_name=\"Validation\"):\n    \"\"\"Comprehensive evaluation of predictions\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"üìä {split_name.upper()} SET EVALUATION\")\n    print(\"=\"*80)\n    \n    # Overall metrics\n    x_rmse = calculate_rmse(y_true[:, 0], y_pred[:, 0])\n    y_rmse = calculate_rmse(y_true[:, 1], y_pred[:, 1])\n    \n    x_mae = calculate_mae(y_true[:, 0], y_pred[:, 0])\n    y_mae = calculate_mae(y_true[:, 1], y_pred[:, 1])\n    \n    # Euclidean distance\n    distances = calculate_euclidean_distance(y_true, y_pred)\n    mean_distance = np.mean(distances)\n    median_distance = np.median(distances)\n    \n    print(f\"\\nüéØ POSITION ACCURACY:\")\n    print(f\"   X-coordinate:\")\n    print(f\"      RMSE: {x_rmse:.3f} yards\")\n    print(f\"      MAE:  {x_mae:.3f} yards\")\n    \n    print(f\"\\n   Y-coordinate:\")\n    print(f\"      RMSE: {y_rmse:.3f} yards\")\n    print(f\"      MAE:  {y_mae:.3f} yards\")\n    \n    print(f\"\\nüìè EUCLIDEAN DISTANCE:\")\n    print(f\"   Mean:   {mean_distance:.3f} yards\")\n    print(f\"   Median: {median_distance:.3f} yards\")\n    print(f\"   Std:    {np.std(distances):.3f} yards\")\n    print(f\"   Min:    {np.min(distances):.3f} yards\")\n    print(f\"   Max:    {np.max(distances):.3f} yards\")\n    \n    # Percentiles\n    print(f\"\\nüìä DISTANCE PERCENTILES:\")\n    for p in [25, 50, 75, 90, 95, 99]:\n        print(f\"   {p}th percentile: {np.percentile(distances, p):.3f} yards\")\n    \n    # Accuracy buckets\n    print(f\"\\nüéØ ACCURACY BUCKETS:\")\n    for threshold in [1, 2, 5, 10, 15, 20]:\n        within = (distances <= threshold).sum()\n        pct = 100 * within / len(distances)\n        print(f\"   Within {threshold:2d} yards: {within:6,} ({pct:5.2f}%)\")\n    \n    metrics = {\n        'x_rmse': x_rmse,\n        'y_rmse': y_rmse,\n        'x_mae': x_mae,\n        'y_mae': y_mae,\n        'mean_distance': mean_distance,\n        'median_distance': median_distance,\n        'distances': distances\n    }\n    \n    return metrics\n\ndef plot_predictions(y_true, y_pred, split_name=\"Validation\", save_path=\"predictions_plot.png\"):\n    \"\"\"Visualize predictions vs actual\"\"\"\n    \n    fig = plt.figure(figsize=(20, 12))\n    \n    # 1. X predictions scatter\n    ax1 = plt.subplot(2, 3, 1)\n    ax1.scatter(y_true[:, 0], y_pred[:, 0], alpha=0.3, s=1)\n    ax1.plot([0, 120], [0, 120], 'r--', linewidth=2)\n    ax1.set_xlabel('Actual X (yards)', fontsize=12)\n    ax1.set_ylabel('Predicted X (yards)', fontsize=12)\n    ax1.set_title(f'{split_name} - X Coordinate', fontsize=14, fontweight='bold')\n    ax1.grid(alpha=0.3)\n    \n    # 2. Y predictions scatter\n    ax2 = plt.subplot(2, 3, 2)\n    ax2.scatter(y_true[:, 1], y_pred[:, 1], alpha=0.3, s=1)\n    ax2.plot([0, 53.3], [0, 53.3], 'r--', linewidth=2)\n    ax2.set_xlabel('Actual Y (yards)', fontsize=12)\n    ax2.set_ylabel('Predicted Y (yards)', fontsize=12)\n    ax2.set_title(f'{split_name} - Y Coordinate', fontsize=14, fontweight='bold')\n    ax2.grid(alpha=0.3)\n    \n    # 3. Error distribution\n    ax3 = plt.subplot(2, 3, 3)\n    distances = calculate_euclidean_distance(y_true, y_pred)\n    ax3.hist(distances, bins=50, alpha=0.7, edgecolor='black')\n    ax3.axvline(np.mean(distances), color='red', linestyle='--', \n                linewidth=2, label=f'Mean: {np.mean(distances):.2f}')\n    ax3.set_xlabel('Euclidean Distance Error (yards)', fontsize=12)\n    ax3.set_ylabel('Frequency', fontsize=12)\n    ax3.set_title('Prediction Error Distribution', fontsize=14, fontweight='bold')\n    ax3.legend()\n    ax3.grid(alpha=0.3)\n    \n    # 4. X error distribution\n    ax4 = plt.subplot(2, 3, 4)\n    x_errors = y_true[:, 0] - y_pred[:, 0]\n    ax4.hist(x_errors, bins=50, alpha=0.7, edgecolor='black', color='green')\n    ax4.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax4.set_xlabel('X Error (yards)', fontsize=12)\n    ax4.set_ylabel('Frequency', fontsize=12)\n    ax4.set_title(f'X Error - Mean: {np.mean(x_errors):.3f}', fontsize=14, fontweight='bold')\n    ax4.grid(alpha=0.3)\n    \n    # 5. Y error distribution\n    ax5 = plt.subplot(2, 3, 5)\n    y_errors = y_true[:, 1] - y_pred[:, 1]\n    ax5.hist(y_errors, bins=50, alpha=0.7, edgecolor='black', color='orange')\n    ax5.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax5.set_xlabel('Y Error (yards)', fontsize=12)\n    ax5.set_ylabel('Frequency', fontsize=12)\n    ax5.set_title(f'Y Error - Mean: {np.mean(y_errors):.3f}', fontsize=14, fontweight='bold')\n    ax5.grid(alpha=0.3)\n    \n    # 6. Cumulative accuracy\n    ax6 = plt.subplot(2, 3, 6)\n    sorted_distances = np.sort(distances)\n    cumulative = np.arange(1, len(sorted_distances) + 1) / len(sorted_distances) * 100\n    ax6.plot(sorted_distances, cumulative, linewidth=2)\n    ax6.set_xlabel('Distance Threshold (yards)', fontsize=12)\n    ax6.set_ylabel('Cumulative % of Predictions', fontsize=12)\n    ax6.set_title('Cumulative Accuracy Curve', fontsize=14, fontweight='bold')\n    ax6.grid(alpha=0.3)\n    \n    # Add benchmarks\n    for threshold in [5, 10, 15]:\n        pct = (distances <= threshold).sum() / len(distances) * 100\n        ax6.axvline(threshold, linestyle='--', alpha=0.5)\n        ax6.text(threshold, pct, f'{pct:.1f}%', fontsize=10)\n    \n    plt.suptitle(f'{split_name} Set - Prediction Analysis', \n                 fontsize=16, fontweight='bold', y=0.995)\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"\\n‚úì Saved plot: {save_path}\")\n    \n    return fig\n\ndef plot_training_history(history, save_path=\"training_history.png\"):\n    \"\"\"Plot training history\"\"\"\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Loss plot\n    axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n    axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n    \n    # MAE plot\n    axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n    axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('MAE (yards)', fontsize=12)\n    axes[1].set_title('Training and Validation MAE', fontsize=14, fontweight='bold')\n    axes[1].legend()\n    axes[1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"‚úì Saved plot: {save_path}\")\n    \n    return fig\n\n# ============================================================================\n# IMPORT FUNCTIONS FROM ORIGINAL CODE\n# ============================================================================\n\ndef parse_height(height_str):\n    if pd.isna(height_str):\n        return np.nan\n    try:\n        feet, inches = map(int, str(height_str).split('-'))\n        return feet * 12 + inches\n    except:\n        return np.nan\n\ndef calculate_age(birth_date, reference_date='2023-09-01'):\n    try:\n        birth = pd.to_datetime(birth_date)\n        ref = pd.to_datetime(reference_date)\n        return (ref - birth).days / 365.25\n    except:\n        return np.nan\n\ndef load_training_data(data_path='/kaggle/input/nfl-big-data-bowl-2026-prediction/train'):\n    print(\"\\n\" + \"=\"*80)\n    print(\"LOADING TRAINING DATA\")\n    print(\"=\"*80)\n    \n    all_data = []\n    for week in range(1, 19):\n        file_path = f'{data_path}/input_2023_w{week:02d}.csv'\n        try:\n            df = pd.read_csv(file_path)\n            all_data.append(df)\n            print(f\"‚úì Week {week:02d}: {len(df):,} rows | {df['play_id'].nunique():,} plays\")\n        except FileNotFoundError:\n            print(f\"‚úó Week {week:02d}: File not found\")\n    \n    train_df = pd.concat(all_data, ignore_index=True)\n    print(f\"\\nTotal training data: {len(train_df):,} rows\")\n    print(f\" Unique plays: {(train_df['game_id'].astype(str) + '_' + train_df['play_id'].astype(str)).nunique():,}\")\n    print(f\"Players to predict: {train_df['player_to_predict'].sum():,}\")\n    \n    return train_df\n\ndef load_test_data():\n    print(\"\\n\" + \"=\"*80)\n    print(\"LOADING TEST DATA\")\n    print(\"=\"*80)\n    \n    test_input = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2026-prediction/test_input.csv')\n    test_targets = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2026-prediction/test.csv')\n    \n    print(f\"‚úì Test input: {len(test_input):,} rows\")\n    print(f\"‚úì Test targets: {len(test_targets):,} predictions needed\")\n    \n    return test_input, test_targets\n\ndef normalize_play_direction(df):\n    df = df.copy()\n    left_mask = df['play_direction'] == 'left'\n    num_flipped = left_mask.sum()\n    \n    df.loc[left_mask, 'x'] = 120 - df.loc[left_mask, 'x']\n    df.loc[left_mask, 'y'] = 53.3 - df.loc[left_mask, 'y']\n    df.loc[left_mask, 'dir'] = (df.loc[left_mask, 'dir'] + 180) % 360\n    df.loc[left_mask, 'o'] = (df.loc[left_mask, 'o'] + 180) % 360\n    \n    if 'ball_land_x' in df.columns:\n        df.loc[left_mask, 'ball_land_x'] = 120 - df.loc[left_mask, 'ball_land_x']\n        df.loc[left_mask, 'ball_land_y'] = 53.3 - df.loc[left_mask, 'ball_land_y']\n    \n    print(f\"   Normalized {num_flipped:,} plays moving left ‚Üí right\")\n    return df\n\ndef engineer_features(df):\n    print(\"\\n\" + \"=\"*80)\n    print(\"FEATURE ENGINEERING\")\n    print(\"=\"*80)\n    \n    df = df.copy()\n    \n    print(\"‚úì Computing velocity components (vx, vy)\")\n    df['vx'] = df['s'] * np.cos(np.radians(df['dir']))\n    df['vy'] = df['s'] * np.sin(np.radians(df['dir']))\n    \n    print(\"‚úì Computing orientation components (ox, oy)\")\n    df['ox'] = np.cos(np.radians(df['o']))\n    df['oy'] = np.sin(np.radians(df['o']))\n    \n    if 'ball_land_x' in df.columns:\n        print(\"‚úì Computing ball landing features\")\n        df['dist_to_ball'] = np.sqrt(\n            (df['x'] - df['ball_land_x'])**2 + \n            (df['y'] - df['ball_land_y'])**2\n        )\n        df['angle_to_ball'] = np.arctan2(\n            df['ball_land_y'] - df['y'],\n            df['ball_land_x'] - df['x']\n        )\n        df['vel_toward_ball'] = df['s'] * np.cos(np.radians(df['dir']) - df['angle_to_ball'])\n    else:\n        df['dist_to_ball'] = 0\n        df['angle_to_ball'] = 0\n        df['vel_toward_ball'] = 0\n    \n    print(\"‚úì Computing field position features\")\n    df['dist_to_left_sideline'] = df['y']\n    df['dist_to_right_sideline'] = 53.3 - df['y']\n    df['dist_to_nearest_sideline'] = np.minimum(df['y'], 53.3 - df['y'])\n    df['dist_to_endzone'] = 120 - df['x']\n    \n    print(\"‚úì Processing player attributes\")\n    df['height_inches'] = df['player_height'].apply(parse_height)\n    df['height_inches'] = df['height_inches'].fillna(df['height_inches'].median())\n    \n    df['player_age'] = df['player_birth_date'].apply(calculate_age)\n    df['player_age'] = df['player_age'].fillna(df['player_age'].median())\n    \n    df['bmi'] = (df['player_weight'] * 703) / (df['height_inches'] ** 2)\n    df['bmi'] = df['bmi'].fillna(df['bmi'].median())\n    \n    print(\"‚úì Creating temporal features (lags, differences)\")\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    group_cols = ['game_id', 'play_id', 'nfl_id']\n    for lag in [1, 2, 3]:\n        for col in ['x', 'y', 's', 'a', 'vx', 'vy']:\n            df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n    \n    df['speed_change'] = df.groupby(group_cols)['s'].diff()\n    df['accel_change'] = df.groupby(group_cols)['a'].diff()\n    df['dir_change'] = df.groupby(group_cols)['dir'].diff()\n    \n    df.loc[df['dir_change'] > 180, 'dir_change'] -= 360\n    df.loc[df['dir_change'] < -180, 'dir_change'] += 360\n    \n    print(\"‚úì Computing rolling statistics\")\n    for col in ['s', 'a']:\n        df[f'{col}_roll_mean'] = df.groupby(group_cols)[col].transform(\n            lambda x: x.rolling(window=3, min_periods=1).mean()\n        )\n        df[f'{col}_roll_std'] = df.groupby(group_cols)[col].transform(\n            lambda x: x.rolling(window=3, min_periods=1).std()\n        )\n    \n    df = df.fillna(method='bfill').fillna(method='ffill').fillna(0)\n    \n    print(f\"\\nüìä Features created: {len(df.columns)} total columns\")\n    \n    return df\n\ndef encode_categorical(df, encoders=None):\n    df = df.copy()\n    categorical_cols = ['player_position', 'player_side', 'player_role']\n    \n    if encoders is None:\n        encoders = {}\n        for col in categorical_cols:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col].astype(str))\n            encoders[col] = le\n        return df, encoders\n    else:\n        for col in categorical_cols:\n            if col in encoders:\n                df[col] = df[col].astype(str).map(\n                    lambda x: x if x in encoders[col].classes_ else encoders[col].classes_[0]\n                )\n                df[col] = encoders[col].transform(df[col])\n        return df\n\ndef create_sequences(df, sequence_length=10, for_training=True):\n    print(\"\\n\" + \"=\"*80)\n    print(\"CREATING SEQUENCES\")\n    print(\"=\"*80)\n    \n    sequence_features = [\n        'x', 'y', 's', 'a', 'vx', 'vy', 'ox', 'oy', 'dir', 'o',\n        'x_lag1', 'y_lag1', 's_lag1', 'a_lag1',\n        'x_lag2', 'y_lag2', 's_lag2', 'a_lag2',\n        'x_lag3', 'y_lag3', 's_lag3', 'a_lag3',\n        'speed_change', 'accel_change', 'dir_change',\n        's_roll_mean', 'a_roll_mean',\n        'dist_to_left_sideline', 'dist_to_right_sideline', 'dist_to_nearest_sideline'\n    ]\n    \n    static_features = [\n        'player_position', 'player_side', 'player_role',\n        'height_inches', 'player_weight', 'player_age', 'bmi',\n        'absolute_yardline_number', 'dist_to_ball', 'angle_to_ball'\n    ]\n    \n    sequences = []\n    static_data = []\n    targets = []\n    metadata = []\n    \n    grouped = df.groupby(['game_id', 'play_id', 'nfl_id'])\n    \n    for (game_id, play_id, nfl_id), group in grouped:\n        if for_training and not group['player_to_predict'].any():\n            continue\n        \n        group = group.sort_values('frame_id')\n        \n        if len(group) < sequence_length:\n            continue\n        \n        seq_data = group[sequence_features].iloc[-sequence_length:].values\n        static = group[static_features].iloc[-1].values\n        \n        sequences.append(seq_data)\n        static_data.append(static)\n        \n        if for_training and 'ball_land_x' in group.columns:\n            target_x = group['ball_land_x'].iloc[-1]\n            target_y = group['ball_land_y'].iloc[-1]\n            targets.append([target_x, target_y])\n        \n        metadata.append({\n            'game_id': game_id,\n            'play_id': play_id,\n            'nfl_id': nfl_id,\n            'num_frames_output': group['num_frames_output'].iloc[-1] if 'num_frames_output' in group.columns else 0,\n            'last_x': group['x'].iloc[-1],\n            'last_y': group['y'].iloc[-1],\n        })\n    \n    sequences = np.array(sequences, dtype=np.float32)\n    static_data = np.array(static_data, dtype=np.float32)\n    \n    if for_training and len(targets) > 0:\n        targets = np.array(targets, dtype=np.float32)\n    else:\n        targets = None\n    \n    print(f\"‚úì Created {len(sequences):,} sequences\")\n    print(f\"‚úì Sequence shape: {sequences.shape}\")\n    print(f\"‚úì Static shape: {static_data.shape}\")\n    if targets is not None:\n        print(f\"‚úì Target shape: {targets.shape}\")\n    \n    return sequences, static_data, targets, metadata\n\ndef build_model(sequence_shape, static_shape):\n    print(\"\\n\" + \"=\"*80)\n    print(\"BUILDING MODEL\")\n    print(\"=\"*80)\n    \n    sequence_input = layers.Input(shape=sequence_shape, name='sequence_input')\n    \n    x = layers.LSTM(128, return_sequences=True)(sequence_input)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    x = layers.LSTM(64, return_sequences=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    static_input = layers.Input(shape=(static_shape,), name='static_input')\n    s = layers.Dense(64, activation='relu')(static_input)\n    s = layers.BatchNormalization()(s)\n    s = layers.Dropout(0.2)(s)\n    s = layers.Dense(32, activation='relu')(s)\n    \n    combined = layers.concatenate([x, s])\n    \n    z = layers.Dense(128, activation='relu')(combined)\n    z = layers.BatchNormalization()(z)\n    z = layers.Dropout(0.3)(z)\n    \n    z = layers.Dense(64, activation='relu')(z)\n    z = layers.Dropout(0.2)(z)\n    \n    # For mixed precision, use float32 output\n    output = layers.Dense(2, dtype='float32', name='position_output')(z)\n    \n    model = keras.Model(\n        inputs=[sequence_input, static_input],\n        outputs=output\n    )\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n        loss='mse',\n        metrics=['mae', 'mse']\n    )\n    \n    model.summary()\n    \n    return model\n\ndef train_model(model, X_seq, X_static, y, validation_split=0.15):\n    print(\"\\n\" + \"=\"*80)\n    print(\"TRAINING MODEL\")\n    print(\"=\"*80)\n    \n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        keras.callbacks.ModelCheckpoint(\n            'best_model.keras',\n            monitor='val_loss',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n    \n    history = model.fit(\n        [X_seq, X_static], y,\n        batch_size=CONFIG['batch_size'],\n        epochs=CONFIG['epochs'],\n        validation_split=validation_split,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    return model, history\n\ndef create_submission(model, test_input, test_targets, metadata_lookup, scalers):\n    print(\"\\n\" + \"=\"*80)\n    print(\"GENERATING PREDICTIONS\")\n    print(\"=\"*80)\n    \n    pred_dict = {}\n    for meta, pred in zip(metadata_lookup, model.predict([test_input[0], test_input[1]], verbose=1)):\n        key = (meta['game_id'], meta['play_id'], meta['nfl_id'])\n        pred_dict[key] = {\n            'x': pred[0],\n            'y': pred[1],\n            'last_x': meta['last_x'],\n            'last_y': meta['last_y']\n        }\n    \n    submissions = []\n    for _, row in test_targets.iterrows():\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        if key in pred_dict:\n            x_pred = pred_dict[key]['x']\n            y_pred = pred_dict[key]['y']\n        else:\n            x_pred = 60.0\n            y_pred = 26.65\n        \n        submissions.append({\n            'id': f\"{row['game_id']}_{row['play_id']}_{row['nfl_id']}_{row['frame_id']}\",\n            'x': x_pred,\n            'y': y_pred\n        })\n    \n    submission_df = pd.DataFrame(submissions)\n    submission_df.to_csv('submission.csv', index=False)\n    \n    print(f\"‚úì Submission created: {len(submission_df):,} predictions\")\n    print(f\"‚úì Saved to: submission.csv\")\n    \n    return submission_df\n\n# ============================================================================\n# MAIN PIPELINE WITH EVALUATION\n# ============================================================================\n\ndef main():\n    start_time = datetime.now()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\" NFL BIG DATA BOWL 2026 - ENHANCED PIPELINE WITH EVALUATION\")\n    print(\"=\"*80)\n    \n    # Setup GPU\n    has_gpu = setup_gpu()\n    \n    # Load data\n    train_df = load_training_data()\n    test_input_df, test_targets_df = load_test_data()\n    \n    # Preprocess\n    print(\"\\nüìç Step 1: Normalizing play direction...\")\n    train_df = normalize_play_direction(train_df)\n    test_input_df = normalize_play_direction(test_input_df)\n    \n    # Feature engineering\n    print(\"\\nüìç Step 2: Feature engineering...\")\n    train_df = engineer_features(train_df)\n    test_input_df = engineer_features(test_input_df)\n    \n    # Encode categorical\n    print(\"\\nüìç Step 3: Encoding categorical variables...\")\n    train_df, encoders = encode_categorical(train_df)\n    test_input_df = encode_categorical(test_input_df, encoders)\n    \n    # Create sequences\n    print(\"\\nüìç Step 4: Creating sequences...\")\n    X_seq_all, X_static_all, y_all, metadata_all = create_sequences(\n        train_df, CONFIG['sequence_length'], for_training=True\n    )\n    \n    X_seq_test, X_static_test, _, metadata_test = create_sequences(\n        test_input_df, CONFIG['sequence_length'], for_training=False\n    )\n    \n    # Split train/validation\n    print(\"\\nüìç Step 5: Splitting train/validation...\")\n    n_samples = len(X_seq_all)\n    n_val = int(n_samples * CONFIG['validation_split'])\n    \n    # Random shuffle\n    indices = np.random.permutation(n_samples)\n    train_idx = indices[n_val:]\n    val_idx = indices[:n_val]\n    \n    X_seq_train = X_seq_all[train_idx]\n    X_static_train = X_static_all[train_idx]\n    y_train = y_all[train_idx]\n    \n    X_seq_val = X_seq_all[val_idx]\n    X_static_val = X_static_all[val_idx]\n    y_val = y_all[val_idx]\n    \n    print(f\"   Training samples: {len(X_seq_train):,}\")\n    print(f\"   Validation samples: {len(X_seq_val):,}\")\n    \n    # Scale features\n    print(\"\\nüìç Step 6: Scaling features...\")\n    scaler_seq = StandardScaler()\n    scaler_static = StandardScaler()\n    \n    # Scale sequence features\n    X_seq_train_flat = X_seq_train.reshape(-1, X_seq_train.shape[-1])\n    X_seq_train_scaled = scaler_seq.fit_transform(X_seq_train_flat).reshape(X_seq_train.shape)\n    \n    X_seq_val_flat = X_seq_val.reshape(-1, X_seq_val.shape[-1])\n    X_seq_val_scaled = scaler_seq.transform(X_seq_val_flat).reshape(X_seq_val.shape)\n    \n    # Scale static features\n    X_static_train_scaled = scaler_static.fit_transform(X_static_train)\n    X_static_val_scaled = scaler_static.transform(X_static_val)\n    \n    # Scale test features\n    X_seq_test_flat = X_seq_test.reshape(-1, X_seq_test.shape[-1])\n    X_seq_test_scaled = scaler_seq.transform(X_seq_test_flat).reshape(X_seq_test.shape)\n    X_static_test_scaled = scaler_static.transform(X_static_test)\n    \n    # Build model\n    print(\"\\nüìç Step 7: Building model...\")\n    model = build_model(\n        sequence_shape=(X_seq_train.shape[1], X_seq_train.shape[2]),\n        static_shape=X_static_train.shape[1]\n    )\n    \n    # Train model WITHOUT validation_split (we already split)\n    print(\"\\nüìç Step 8: Training model...\")\n    \n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        keras.callbacks.ModelCheckpoint(\n            'best_model.keras',\n            monitor='val_loss',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n    \n    history = model.fit(\n        [X_seq_train_scaled, X_static_train_scaled], y_train,\n        validation_data=([X_seq_val_scaled, X_static_val_scaled], y_val),\n        batch_size=CONFIG['batch_size'],\n        epochs=CONFIG['epochs'],\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Plot training history\n    print(\"\\nüìç Step 9: Plotting training history...\")\n    plot_training_history(history, \"training_history.png\")\n    \n    # Evaluate on training set\n    print(\"\\nüìç Step 10: Evaluating on training set...\")\n    y_train_pred = model.predict([X_seq_train_scaled, X_static_train_scaled], verbose=0)\n    train_metrics = evaluate_predictions(y_train, y_train_pred, \"Training\")\n    plot_predictions(y_train, y_train_pred, \"Training\", \"predictions_train.png\")\n    \n    # Evaluate on validation set\n    print(\"\\nüìç Step 11: Evaluating on validation set...\")\n    y_val_pred = model.predict([X_seq_val_scaled, X_static_val_scaled], verbose=0)\n    val_metrics = evaluate_predictions(y_val, y_val_pred, \"Validation\")\n    plot_predictions(y_val, y_val_pred, \"Validation\", \"predictions_val.png\")\n    \n    # Save model and artifacts\n    print(\"\\nüìç Step 12: Saving model and artifacts...\")\n    model.save('nfl_model_final.keras')\n    with open('scalers.pkl', 'wb') as f:\n        pickle.dump({'seq': scaler_seq, 'static': scaler_static, 'encoders': encoders}, f)\n    \n    # Save metrics to file\n    metrics_summary = {\n        'training': {\n            'x_rmse': float(train_metrics['x_rmse']),\n            'y_rmse': float(train_metrics['y_rmse']),\n            'x_mae': float(train_metrics['x_mae']),\n            'y_mae': float(train_metrics['y_mae']),\n            'mean_distance': float(train_metrics['mean_distance']),\n            'median_distance': float(train_metrics['median_distance'])\n        },\n        'validation': {\n            'x_rmse': float(val_metrics['x_rmse']),\n            'y_rmse': float(val_metrics['y_rmse']),\n            'x_mae': float(val_metrics['x_mae']),\n            'y_mae': float(val_metrics['y_mae']),\n            'mean_distance': float(val_metrics['mean_distance']),\n            'median_distance': float(val_metrics['median_distance'])\n        }\n    }\n    \n    with open('metrics.pkl', 'wb') as f:\n        pickle.dump(metrics_summary, f)\n    \n    # Create submission\n    print(\"\\nüìç Step 13: Creating submission...\")\n    submission = create_submission(\n        model, \n        (X_seq_test_scaled, X_static_test_scaled),\n        test_targets_df,\n        metadata_test,\n        {'seq': scaler_seq, 'static': scaler_static}\n    )\n    \n    # Final summary\n    end_time = datetime.now()\n    duration = end_time - start_time\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"‚úÖ PIPELINE COMPLETE!\")\n    print(\"=\"*80)\n    \n    print(f\"\\n‚è±Ô∏è  Total Time: {duration}\")\n    \n    print(f\"\\nüìÅ Files created:\")\n    print(f\"   ‚Ä¢ nfl_model_final.keras - Trained model\")\n    print(f\"   ‚Ä¢ best_model.keras - Best model checkpoint\")\n    print(f\"   ‚Ä¢ scalers.pkl - Feature scalers and encoders\")\n    print(f\"   ‚Ä¢ metrics.pkl - Evaluation metrics\")\n    print(f\"   ‚Ä¢ submission.csv - Final predictions ({len(submission):,} rows)\")\n    print(f\"   ‚Ä¢ training_history.png - Training curves\")\n    print(f\"   ‚Ä¢ predictions_train.png - Training set predictions\")\n    print(f\"   ‚Ä¢ predictions_val.png - Validation set predictions\")\n    \n    print(f\"\\nüìä FINAL RESULTS:\")\n    print(f\"\\n   Training Set:\")\n    print(f\"      RMSE (X): {train_metrics['x_rmse']:.3f} yards\")\n    print(f\"      RMSE (Y): {train_metrics['y_rmse']:.3f} yards\")\n    print(f\"      Mean Distance Error: {train_metrics['mean_distance']:.3f} yards\")\n    \n    print(f\"\\n   Validation Set:\")\n    print(f\"      RMSE (X): {val_metrics['x_rmse']:.3f} yards\")\n    print(f\"      RMSE (Y): {val_metrics['y_rmse']:.3f} yards\")\n    print(f\"      Mean Distance Error: {val_metrics['mean_distance']:.3f} yards\")\n    \n    print(f\"\\nüéØ Model Performance Summary:\")\n    within_5_val = (val_metrics['distances'] <= 5).sum() / len(val_metrics['distances']) * 100\n    within_10_val = (val_metrics['distances'] <= 10).sum() / len(val_metrics['distances']) * 100\n    print(f\"   Predictions within 5 yards: {within_5_val:.1f}%\")\n    print(f\"   Predictions within 10 yards: {within_10_val:.1f}%\")\n    \n    print(\"\\n\" + \"=\"*80)\n    \n    return model, history, submission, train_metrics, val_metrics\n\n# ============================================================================\n# RUN\n# ============================================================================\n\nif __name__ == \"__main__\":\n    model, history, submission, train_metrics, val_metrics = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:37:54.061167Z","iopub.execute_input":"2025-09-30T16:37:54.061496Z","iopub.status.idle":"2025-09-30T17:59:56.11956Z","shell.execute_reply.started":"2025-09-30T16:37:54.061471Z","shell.execute_reply":"2025-09-30T17:59:56.11803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split,StratifiedShuffleSplit,GroupShuffleSplit\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pickle\nimport warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\nfrom datetime import datetime\nimport seaborn as sns\nfrom scipy.stats import skew, kurtosis\nfrom scipy import stats\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport lightgbm as lgb\nimport optuna\nimport joblib\nimport numpy as np\nfrom sklearn.model_selection import KFold\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T03:12:42.779572Z","iopub.execute_input":"2025-10-15T03:12:42.779914Z","iopub.status.idle":"2025-10-15T03:12:42.786597Z","shell.execute_reply.started":"2025-10-15T03:12:42.779887Z","shell.execute_reply":"2025-10-15T03:12:42.785595Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# ============================================================================\n# GPU CONFIGURATION & OPTIMIZATION\n# ============================================================================\n\ndef setup_gpu():\n    \"\"\"Configure GPU for optimal performance\"\"\"\n    print(\"=\"*80)\n    print(\"GPU CONFIGURATION\")\n    print(\"=\"*80)\n    \n    # Check available GPUs\n    gpus = tf.config.list_physical_devices('GPU')\n    print(f\"\\nüñ•Ô∏è  Available GPUs: {len(gpus)}\")\n    \n    if gpus:\n        try:\n            # Enable memory growth (don't allocate all GPU memory at once)\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n                print(f\"   ‚úì GPU: {gpu.name} - Memory growth enabled\")\n            \n            # Set GPU memory limit (optional - useful if sharing GPU)\n            # tf.config.set_logical_device_configuration(\n            #     gpus[0],\n            #     [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  # 4GB\n            # )\n            \n            # Use mixed precision for faster training\n            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n            tf.keras.mixed_precision.set_global_policy(policy)\n            print(f\"   ‚úì Mixed precision enabled: {policy.name}\")\n            \n            logical_gpus = tf.config.list_logical_devices('GPU')\n            print(f\"   ‚úì Logical GPUs: {len(logical_gpus)}\")\n            \n        except RuntimeError as e:\n            print(f\"   ‚ö†Ô∏è  GPU configuration error: {e}\")\n    else:\n        print(\"   ‚ö†Ô∏è  No GPU found - using CPU (training will be slower)\")\n    \n    # Set TensorFlow options for better performance\n    tf.config.optimizer.set_jit(True)  # XLA compilation\n    print(\"   ‚úì XLA (Accelerated Linear Algebra) enabled\")\n    \n    print(f\"\\nüìä TensorFlow version: {tf.__version__}\")\n    print(f\"üìä Keras version: {keras.__version__}\")\n    \n    return len(gpus) > 0\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nCONFIG = {\n    'sequence_length': 10,\n    'max_frames_to_predict': 15,\n    'batch_size': 256,  # Larger batch for GPU\n    'epochs': 100,\n    'learning_rate': 0.001,\n    'validation_split': 0.15,\n    'use_gpu': True,\n}\n\n# ============================================================================\n# EVALUATION METRICS\n# ============================================================================\n\ndef calculate_rmse(y_true, y_pred):\n    \"\"\"Calculate Root Mean Squared Error\"\"\"\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n    return rmse\n\ndef calculate_mae(y_true, y_pred):\n    \"\"\"Calculate Mean Absolute Error\"\"\"\n    mae = np.mean(np.abs(y_true - y_pred))\n    return mae\n\ndef calculate_euclidean_distance(y_true, y_pred):\n    \"\"\"Calculate Euclidean distance between predicted and actual positions\"\"\"\n    distances = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 + \n                       (y_true[:, 1] - y_pred[:, 1])**2)\n    return distances\n\ndef evaluate_predictions(y_true, y_pred, split_name=\"Validation\"):\n    \"\"\"Comprehensive evaluation of predictions\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"üìä {split_name.upper()} SET EVALUATION\")\n    print(\"=\"*80)\n    \n    # Overall metrics\n    x_rmse = calculate_rmse(y_true[:, 0], y_pred[:, 0])\n    y_rmse = calculate_rmse(y_true[:, 1], y_pred[:, 1])\n    \n    x_mae = calculate_mae(y_true[:, 0], y_pred[:, 0])\n    y_mae = calculate_mae(y_true[:, 1], y_pred[:, 1])\n    \n    # Euclidean distance\n    distances = calculate_euclidean_distance(y_true, y_pred)\n    mean_distance = np.mean(distances)\n    median_distance = np.median(distances)\n    \n    print(f\"\\nüéØ POSITION ACCURACY:\")\n    print(f\"   X-coordinate:\")\n    print(f\"      RMSE: {x_rmse:.3f} yards\")\n    print(f\"      MAE:  {x_mae:.3f} yards\")\n    \n    print(f\"\\n   Y-coordinate:\")\n    print(f\"      RMSE: {y_rmse:.3f} yards\")\n    print(f\"      MAE:  {y_mae:.3f} yards\")\n    \n    print(f\"\\nüìè EUCLIDEAN DISTANCE:\")\n    print(f\"   Mean:   {mean_distance:.3f} yards\")\n    print(f\"   Median: {median_distance:.3f} yards\")\n    print(f\"   Std:    {np.std(distances):.3f} yards\")\n    print(f\"   Min:    {np.min(distances):.3f} yards\")\n    print(f\"   Max:    {np.max(distances):.3f} yards\")\n    \n    # Percentiles\n    print(f\"\\nüìä DISTANCE PERCENTILES:\")\n    for p in [25, 50, 75, 90, 95, 99]:\n        print(f\"   {p}th percentile: {np.percentile(distances, p):.3f} yards\")\n    \n    # Accuracy buckets\n    print(f\"\\nüéØ ACCURACY BUCKETS:\")\n    for threshold in [1, 2, 5, 10, 15, 20]:\n        within = (distances <= threshold).sum()\n        pct = 100 * within / len(distances)\n        print(f\"   Within {threshold:2d} yards: {within:6,} ({pct:5.2f}%)\")\n    \n    metrics = {\n        'x_rmse': x_rmse,\n        'y_rmse': y_rmse,\n        'x_mae': x_mae,\n        'y_mae': y_mae,\n        'mean_distance': mean_distance,\n        'median_distance': median_distance,\n        'distances': distances\n    }\n    \n    return metrics\n\ndef plot_predictions(y_true, y_pred, split_name=\"Validation\", save_path=\"predictions_plot.png\"):\n    \"\"\"Visualize predictions vs actual\"\"\"\n    \n    fig = plt.figure(figsize=(20, 12))\n    \n    # 1. X predictions scatter\n    ax1 = plt.subplot(2, 3, 1)\n    ax1.scatter(y_true[:, 0], y_pred[:, 0], alpha=0.3, s=1)\n    ax1.plot([0, 120], [0, 120], 'r--', linewidth=2)\n    ax1.set_xlabel('Actual X (yards)', fontsize=12)\n    ax1.set_ylabel('Predicted X (yards)', fontsize=12)\n    ax1.set_title(f'{split_name} - X Coordinate', fontsize=14, fontweight='bold')\n    ax1.grid(alpha=0.3)\n    \n    # 2. Y predictions scatter\n    ax2 = plt.subplot(2, 3, 2)\n    ax2.scatter(y_true[:, 1], y_pred[:, 1], alpha=0.3, s=1)\n    ax2.plot([0, 53.3], [0, 53.3], 'r--', linewidth=2)\n    ax2.set_xlabel('Actual Y (yards)', fontsize=12)\n    ax2.set_ylabel('Predicted Y (yards)', fontsize=12)\n    ax2.set_title(f'{split_name} - Y Coordinate', fontsize=14, fontweight='bold')\n    ax2.grid(alpha=0.3)\n    \n    # 3. Error distribution\n    ax3 = plt.subplot(2, 3, 3)\n    distances = calculate_euclidean_distance(y_true, y_pred)\n    ax3.hist(distances, bins=50, alpha=0.7, edgecolor='black')\n    ax3.axvline(np.mean(distances), color='red', linestyle='--', \n                linewidth=2, label=f'Mean: {np.mean(distances):.2f}')\n    ax3.set_xlabel('Euclidean Distance Error (yards)', fontsize=12)\n    ax3.set_ylabel('Frequency', fontsize=12)\n    ax3.set_title('Prediction Error Distribution', fontsize=14, fontweight='bold')\n    ax3.legend()\n    ax3.grid(alpha=0.3)\n    \n    # 4. X error distribution\n    ax4 = plt.subplot(2, 3, 4)\n    x_errors = y_true[:, 0] - y_pred[:, 0]\n    ax4.hist(x_errors, bins=50, alpha=0.7, edgecolor='black', color='green')\n    ax4.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax4.set_xlabel('X Error (yards)', fontsize=12)\n    ax4.set_ylabel('Frequency', fontsize=12)\n    ax4.set_title(f'X Error - Mean: {np.mean(x_errors):.3f}', fontsize=14, fontweight='bold')\n    ax4.grid(alpha=0.3)\n    \n    # 5. Y error distribution\n    ax5 = plt.subplot(2, 3, 5)\n    y_errors = y_true[:, 1] - y_pred[:, 1]\n    ax5.hist(y_errors, bins=50, alpha=0.7, edgecolor='black', color='orange')\n    ax5.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax5.set_xlabel('Y Error (yards)', fontsize=12)\n    ax5.set_ylabel('Frequency', fontsize=12)\n    ax5.set_title(f'Y Error - Mean: {np.mean(y_errors):.3f}', fontsize=14, fontweight='bold')\n    ax5.grid(alpha=0.3)\n    \n    # 6. Cumulative accuracy\n    ax6 = plt.subplot(2, 3, 6)\n    sorted_distances = np.sort(distances)\n    cumulative = np.arange(1, len(sorted_distances) + 1) / len(sorted_distances) * 100\n    ax6.plot(sorted_distances, cumulative, linewidth=2)\n    ax6.set_xlabel('Distance Threshold (yards)', fontsize=12)\n    ax6.set_ylabel('Cumulative % of Predictions', fontsize=12)\n    ax6.set_title('Cumulative Accuracy Curve', fontsize=14, fontweight='bold')\n    ax6.grid(alpha=0.3)\n    \n    # Add benchmarks\n    for threshold in [5, 10, 15]:\n        pct = (distances <= threshold).sum() / len(distances) * 100\n        ax6.axvline(threshold, linestyle='--', alpha=0.5)\n        ax6.text(threshold, pct, f'{pct:.1f}%', fontsize=10)\n    \n    plt.suptitle(f'{split_name} Set - Prediction Analysis', \n                 fontsize=16, fontweight='bold', y=0.995)\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"\\n‚úì Saved plot: {save_path}\")\n    \n    return fig\n\ndef plot_training_history(history, save_path=\"training_history.png\"):\n    \"\"\"Plot training history\"\"\"\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Loss plot\n    axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n    axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n    \n    # MAE plot\n    axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n    axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('MAE (yards)', fontsize=12)\n    axes[1].set_title('Training and Validation MAE', fontsize=14, fontweight='bold')\n    axes[1].legend()\n    axes[1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"‚úì Saved plot: {save_path}\")\n    \n    return fig\n\n# ============================================================================\n# IMPORT FUNCTIONS FROM ORIGINAL CODE\n# ============================================================================\n\ndef parse_height(height_str):\n    if pd.isna(height_str):\n        return np.nan\n    try:\n        feet, inches = map(int, str(height_str).split('-'))\n        return feet * 12 + inches\n    except:\n        return np.nan\n\ndef calculate_age(birth_date, reference_date='2023-09-01'):\n    try:\n        birth = pd.to_datetime(birth_date)\n        ref = pd.to_datetime(reference_date)\n        return (ref - birth).days / 365.25\n    except:\n        return np.nan\n\ndef load_training_data(data_path='/kaggle/input/nfl-big-data-bowl-2026-prediction/train'):\n    print(\"\\n\" + \"=\"*80)\n    print(\"LOADING TRAINING DATA\")\n    print(\"=\"*80)\n    \n    all_data = []\n    for week in range(1, 19):\n        file_path = f'{data_path}/input_2023_w{week:02d}.csv'\n        try:\n            df = pd.read_csv(file_path)\n            all_data.append(df)\n            print(f\"‚úì Week {week:02d}: {len(df):,} rows | {df['play_id'].nunique():,} plays\")\n        except FileNotFoundError:\n            print(f\"‚úó Week {week:02d}: File not found\")\n    \n    train_df = pd.concat(all_data, ignore_index=True)\n    print(f\"\\nTotal training data: {len(train_df):,} rows\")\n    print(f\" Unique plays: {(train_df['game_id'].astype(str) + '_' + train_df['play_id'].astype(str)).nunique():,}\")\n    print(f\"Players to predict: {train_df['player_to_predict'].sum():,}\")\n    \n    return train_df\n\ndef load_test_data():\n    print(\"\\n\" + \"=\"*80)\n    print(\"LOADING TEST DATA\")\n    print(\"=\"*80)\n    \n    test_input = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2026-prediction/test_input.csv')\n    test_targets = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2026-prediction/test.csv')\n    \n    print(f\"‚úì Test input: {len(test_input):,} rows\")\n    print(f\"‚úì Test targets: {len(test_targets):,} predictions needed\")\n    \n    return test_input, test_targets\n\ndef normalize_play_direction(df):\n    df = df.copy()\n    left_mask = df['play_direction'] == 'left'\n    num_flipped = left_mask.sum()\n    \n    df.loc[left_mask, 'x'] = 120 - df.loc[left_mask, 'x']\n    df.loc[left_mask, 'y'] = 53.3 - df.loc[left_mask, 'y']\n    df.loc[left_mask, 'dir'] = (df.loc[left_mask, 'dir'] + 180) % 360\n    df.loc[left_mask, 'o'] = (df.loc[left_mask, 'o'] + 180) % 360\n    \n    if 'ball_land_x' in df.columns:\n        df.loc[left_mask, 'ball_land_x'] = 120 - df.loc[left_mask, 'ball_land_x']\n        df.loc[left_mask, 'ball_land_y'] = 53.3 - df.loc[left_mask, 'ball_land_y']\n    \n    print(f\"   Normalized {num_flipped:,} plays moving left ‚Üí right\")\n    return df\n\ndef engineer_features(df):\n    print(\"\\n\" + \"=\"*80)\n    print(\"FEATURE ENGINEERING\")\n    print(\"=\"*80)\n    \n    df = df.copy()\n    \n    print(\"‚úì Computing velocity components (vx, vy)\")\n    df['vx'] = df['s'] * np.cos(np.radians(df['dir']))\n    df['vy'] = df['s'] * np.sin(np.radians(df['dir']))\n    \n    print(\"‚úì Computing orientation components (ox, oy)\")\n    df['ox'] = np.cos(np.radians(df['o']))\n    df['oy'] = np.sin(np.radians(df['o']))\n    \n    if 'ball_land_x' in df.columns:\n        print(\"‚úì Computing ball landing features\")\n        df['dist_to_ball'] = np.sqrt(\n            (df['x'] - df['ball_land_x'])**2 + \n            (df['y'] - df['ball_land_y'])**2\n        )\n        df['angle_to_ball'] = np.arctan2(\n            df['ball_land_y'] - df['y'],\n            df['ball_land_x'] - df['x']\n        )\n        df['vel_toward_ball'] = df['s'] * np.cos(np.radians(df['dir']) - df['angle_to_ball'])\n    else:\n        df['dist_to_ball'] = 0\n        df['angle_to_ball'] = 0\n        df['vel_toward_ball'] = 0\n    \n    print(\"‚úì Computing field position features\")\n    df['dist_to_left_sideline'] = df['y']\n    df['dist_to_right_sideline'] = 53.3 - df['y']\n    df['dist_to_nearest_sideline'] = np.minimum(df['y'], 53.3 - df['y'])\n    df['dist_to_endzone'] = 120 - df['x']\n    \n    print(\"‚úì Processing player attributes\")\n    df['height_inches'] = df['player_height'].apply(parse_height)\n    df['height_inches'] = df['height_inches'].fillna(df['height_inches'].median())\n    \n    df['player_age'] = df['player_birth_date'].apply(calculate_age)\n    df['player_age'] = df['player_age'].fillna(df['player_age'].median())\n    \n    df['bmi'] = (df['player_weight'] * 703) / (df['height_inches'] ** 2)\n    df['bmi'] = df['bmi'].fillna(df['bmi'].median())\n    \n    print(\"‚úì Creating temporal features (lags, differences)\")\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    group_cols = ['game_id', 'play_id', 'nfl_id']\n    for lag in [1, 2, 3]:\n        for col in ['x', 'y', 's', 'a', 'vx', 'vy']:\n            df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n    \n    df['speed_change'] = df.groupby(group_cols)['s'].diff()\n    df['accel_change'] = df.groupby(group_cols)['a'].diff()\n    df['dir_change'] = df.groupby(group_cols)['dir'].diff()\n    \n    df.loc[df['dir_change'] > 180, 'dir_change'] -= 360\n    df.loc[df['dir_change'] < -180, 'dir_change'] += 360\n    \n    print(\"‚úì Computing rolling statistics\")\n    for col in ['s', 'a']:\n        df[f'{col}_roll_mean'] = df.groupby(group_cols)[col].transform(\n            lambda x: x.rolling(window=3, min_periods=1).mean()\n        )\n        df[f'{col}_roll_std'] = df.groupby(group_cols)[col].transform(\n            lambda x: x.rolling(window=3, min_periods=1).std()\n        )\n    \n    df = df.fillna(method='bfill').fillna(method='ffill').fillna(0)\n    \n    print(f\"\\nüìä Features created: {len(df.columns)} total columns\")\n    \n    return df\n\ndef encode_categorical(df, encoders=None):\n    df = df.copy()\n    categorical_cols = ['player_position', 'player_side', 'player_role']\n    \n    if encoders is None:\n        encoders = {}\n        for col in categorical_cols:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col].astype(str))\n            encoders[col] = le\n        return df, encoders\n    else:\n        for col in categorical_cols:\n            if col in encoders:\n                df[col] = df[col].astype(str).map(\n                    lambda x: x if x in encoders[col].classes_ else encoders[col].classes_[0]\n                )\n                df[col] = encoders[col].transform(df[col])\n        return df\n\ndef create_sequences(df, sequence_length=10, for_training=True):\n    print(\"\\n\" + \"=\"*80)\n    print(\"CREATING SEQUENCES\")\n    print(\"=\"*80)\n    \n    sequence_features = [\n        'x', 'y', 's', 'a', 'vx', 'vy', 'ox', 'oy', 'dir', 'o',\n        'x_lag1', 'y_lag1', 's_lag1', 'a_lag1',\n        'x_lag2', 'y_lag2', 's_lag2', 'a_lag2',\n        'x_lag3', 'y_lag3', 's_lag3', 'a_lag3',\n        'speed_change', 'accel_change', 'dir_change',\n        's_roll_mean', 'a_roll_mean',\n        'dist_to_left_sideline', 'dist_to_right_sideline', 'dist_to_nearest_sideline'\n    ]\n    \n    static_features = [\n        'player_position', 'player_side', 'player_role',\n        'height_inches', 'player_weight', 'player_age', 'bmi',\n        'absolute_yardline_number', 'dist_to_ball', 'angle_to_ball'\n    ]\n    \n    sequences = []\n    static_data = []\n    targets = []\n    metadata = []\n    \n    grouped = df.groupby(['game_id', 'play_id', 'nfl_id'])\n    \n    for (game_id, play_id, nfl_id), group in grouped:\n        if for_training and not group['player_to_predict'].any():\n            continue\n        \n        group = group.sort_values('frame_id')\n        \n        if len(group) < sequence_length:\n            continue\n        \n        seq_data = group[sequence_features].iloc[-sequence_length:].values\n        static = group[static_features].iloc[-1].values\n        \n        sequences.append(seq_data)\n        static_data.append(static)\n        \n        if for_training and 'ball_land_x' in group.columns:\n            target_x = group['ball_land_x'].iloc[-1]\n            target_y = group['ball_land_y'].iloc[-1]\n            targets.append([target_x, target_y])\n        \n        metadata.append({\n            'game_id': game_id,\n            'play_id': play_id,\n            'nfl_id': nfl_id,\n            'num_frames_output': group['num_frames_output'].iloc[-1] if 'num_frames_output' in group.columns else 0,\n            'last_x': group['x'].iloc[-1],\n            'last_y': group['y'].iloc[-1],\n        })\n    \n    sequences = np.array(sequences, dtype=np.float32)\n    static_data = np.array(static_data, dtype=np.float32)\n    \n    if for_training and len(targets) > 0:\n        targets = np.array(targets, dtype=np.float32)\n    else:\n        targets = None\n    \n    print(f\"‚úì Created {len(sequences):,} sequences\")\n    print(f\"‚úì Sequence shape: {sequences.shape}\")\n    print(f\"‚úì Static shape: {static_data.shape}\")\n    if targets is not None:\n        print(f\"‚úì Target shape: {targets.shape}\")\n    \n    return sequences, static_data, targets, metadata\n\ndef build_model(sequence_shape, static_shape):\n    print(\"\\n\" + \"=\"*80)\n    print(\"BUILDING MODEL\")\n    print(\"=\"*80)\n    \n    sequence_input = layers.Input(shape=sequence_shape, name='sequence_input')\n    \n    x = layers.LSTM(128, return_sequences=True)(sequence_input)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    x = layers.LSTM(64, return_sequences=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    static_input = layers.Input(shape=(static_shape,), name='static_input')\n    s = layers.Dense(64, activation='relu')(static_input)\n    s = layers.BatchNormalization()(s)\n    s = layers.Dropout(0.2)(s)\n    s = layers.Dense(32, activation='relu')(s)\n    \n    combined = layers.concatenate([x, s])\n    \n    z = layers.Dense(128, activation='relu')(combined)\n    z = layers.BatchNormalization()(z)\n    z = layers.Dropout(0.3)(z)\n    \n    z = layers.Dense(64, activation='relu')(z)\n    z = layers.Dropout(0.2)(z)\n    \n    # For mixed precision, use float32 output\n    output = layers.Dense(2, dtype='float32', name='position_output')(z)\n    \n    model = keras.Model(\n        inputs=[sequence_input, static_input],\n        outputs=output\n    )\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n        loss='mse',\n        metrics=['mae', 'mse']\n    )\n    \n    model.summary()\n    \n    return model\n\ndef train_model(model, X_seq, X_static, y, validation_split=0.15):\n    print(\"\\n\" + \"=\"*80)\n    print(\"TRAINING MODEL\")\n    print(\"=\"*80)\n    \n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        keras.callbacks.ModelCheckpoint(\n            'best_model.keras',\n            monitor='val_loss',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n    \n    history = model.fit(\n        [X_seq, X_static], y,\n        batch_size=CONFIG['batch_size'],\n        epochs=CONFIG['epochs'],\n        validation_split=validation_split,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    return model, history\n\ndef create_submission(model, test_input, test_targets, metadata_lookup, scalers):\n    print(\"\\n\" + \"=\"*80)\n    print(\"GENERATING PREDICTIONS\")\n    print(\"=\"*80)\n    \n    pred_dict = {}\n    for meta, pred in zip(metadata_lookup, model.predict([test_input[0], test_input[1]], verbose=1)):\n        key = (meta['game_id'], meta['play_id'], meta['nfl_id'])\n        pred_dict[key] = {\n            'x': pred[0],\n            'y': pred[1],\n            'last_x': meta['last_x'],\n            'last_y': meta['last_y']\n        }\n    \n    submissions = []\n    for _, row in test_targets.iterrows():\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        if key in pred_dict:\n            x_pred = pred_dict[key]['x']\n            y_pred = pred_dict[key]['y']\n        else:\n            x_pred = 60.0\n            y_pred = 26.65\n        \n        submissions.append({\n            'id': f\"{row['game_id']}_{row['play_id']}_{row['nfl_id']}_{row['frame_id']}\",\n            'x': x_pred,\n            'y': y_pred\n        })\n    \n    submission_df = pd.DataFrame(submissions)\n    submission_df.to_csv('submission.csv', index=False)\n    \n    print(f\"‚úì Submission created: {len(submission_df):,} predictions\")\n    print(f\"‚úì Saved to: submission.csv\")\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T03:12:44.00224Z","iopub.execute_input":"2025-10-15T03:12:44.002533Z","iopub.status.idle":"2025-10-15T03:12:44.09005Z","shell.execute_reply.started":"2025-10-15T03:12:44.002514Z","shell.execute_reply":"2025-10-15T03:12:44.089075Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"### ENTENDIMIENTO DEL PROBLEMA\n\n1. **Objetivo de competencia:** predecir el movimiento del bal√≥n y jugadores tras el lanzamiento.\n2. **Que tipo de problema es:** Regresion multivariable temporal\n3. **Representacion del dataset:** mediciones por frames de jugadas\n4. **Variables Representativas:** posici√≥n (x, y), velocidad (s), aceleraci√≥n (a), orientaci√≥n (dir), tipo de jugador, etc.\n5. **Objetivo a llegar:** un modelo que generalice bien y capture la din√°mica del juego.","metadata":{}},{"cell_type":"code","source":"# Load data\ntrain_df = load_training_data()\ntest_input_df, test_targets_df = load_test_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T03:12:54.161715Z","iopub.execute_input":"2025-10-15T03:12:54.162053Z","iopub.status.idle":"2025-10-15T03:13:23.033595Z","shell.execute_reply.started":"2025-10-15T03:12:54.162029Z","shell.execute_reply":"2025-10-15T03:13:23.032595Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nLOADING TRAINING DATA\n================================================================================\n‚úì Week 01: 285,714 rows | 748 plays\n‚úì Week 02: 288,586 rows | 777 plays\n‚úì Week 03: 297,757 rows | 823 plays\n‚úì Week 04: 272,475 rows | 710 plays\n‚úì Week 05: 254,779 rows | 677 plays\n‚úì Week 06: 270,676 rows | 715 plays\n‚úì Week 07: 233,597 rows | 646 plays\n‚úì Week 08: 281,011 rows | 765 plays\n‚úì Week 09: 252,796 rows | 656 plays\n‚úì Week 10: 260,372 rows | 673 plays\n‚úì Week 11: 243,413 rows | 657 plays\n‚úì Week 12: 294,940 rows | 755 plays\n‚úì Week 13: 233,755 rows | 622 plays\n‚úì Week 14: 279,972 rows | 738 plays\n‚úì Week 15: 281,820 rows | 702 plays\n‚úì Week 16: 316,417 rows | 822 plays\n‚úì Week 17: 277,582 rows | 734 plays\n‚úì Week 18: 254,917 rows | 686 plays\n\nTotal training data: 4,880,579 rows\n Unique plays: 14,108\nPlayers to predict: 1,303,440\n\n================================================================================\nLOADING TEST DATA\n================================================================================\n‚úì Test input: 49,753 rows\n‚úì Test targets: 5,837 predictions needed\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T03:13:23.035144Z","iopub.execute_input":"2025-10-15T03:13:23.035499Z","iopub.status.idle":"2025-10-15T03:13:23.052281Z","shell.execute_reply.started":"2025-10-15T03:13:23.035465Z","shell.execute_reply":"2025-10-15T03:13:23.051093Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4880579 entries, 0 to 4880578\nData columns (total 23 columns):\n #   Column                    Dtype  \n---  ------                    -----  \n 0   game_id                   int64  \n 1   play_id                   int64  \n 2   player_to_predict         bool   \n 3   nfl_id                    int64  \n 4   frame_id                  int64  \n 5   play_direction            object \n 6   absolute_yardline_number  int64  \n 7   player_name               object \n 8   player_height             object \n 9   player_weight             int64  \n 10  player_birth_date         object \n 11  player_position           object \n 12  player_side               object \n 13  player_role               object \n 14  x                         float64\n 15  y                         float64\n 16  s                         float64\n 17  a                         float64\n 18  dir                       float64\n 19  o                         float64\n 20  num_frames_output         int64  \n 21  ball_land_x               float64\n 22  ball_land_y               float64\ndtypes: bool(1), float64(8), int64(7), object(7)\nmemory usage: 823.8+ MB\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"| Tipo      | Variable                                                                 | Descripci√≥n                                                    |\n| ---------- | ------------------------------------------------------------------------ | -------------------------------------------------------------- |\n| Entrada   | `game_id`                                                                | Identificador √∫nico del partido                                |\n| Entrada   | `play_id`                                                                | Identificador de la jugada dentro del partido                  |\n| Entrada   | `nfl_id`                                                                 | Identificador del jugador                                      |\n| Entrada   | `frame_id`                                                               | N√∫mero de frame (instant√°neo temporal dentro de la jugada)     |\n| Entrada   | `play_direction`                                                         | Direcci√≥n del avance de la jugada (‚Äúleft‚Äù o ‚Äúright‚Äù)           |\n| Entrada   | `absolute_yardline_number`                                               | Posici√≥n absoluta en el campo (yardas)                         |\n| Entrada   | `x`, `y`                                                                 | Coordenadas del jugador en el campo                            |\n| Entrada   | `s`, `a`                                                                 | Velocidad y aceleraci√≥n instant√°nea                            |\n| Entrada   | `dir`, `o`                                                               | Direcci√≥n y orientaci√≥n del jugador (en grados)                |\n| Entrada   | `player_height`, `player_weight`, `player_birth_date`, `player_position` | Atributos f√≠sicos                                              |\n| Salida    | `ball_land_x`, `ball_land_y`                                             | Coordenadas donde aterriza el bal√≥n (lo que se desea predecir) |\n| Indicador | `player_to_predict`                                                      | Indica si el jugador debe ser considerado para la predicci√≥n   |\n","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T03:13:23.053486Z","iopub.execute_input":"2025-10-15T03:13:23.054147Z","iopub.status.idle":"2025-10-15T03:13:25.712435Z","shell.execute_reply.started":"2025-10-15T03:13:23.054116Z","shell.execute_reply":"2025-10-15T03:13:25.711349Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"            game_id       play_id        nfl_id      frame_id  \\\ncount  4.880579e+06  4.880579e+06  4.880579e+06  4.880579e+06   \nmean   2.023155e+09  2.196409e+03  4.955890e+04  1.613179e+01   \nstd    2.011405e+05  1.246426e+03  5.210338e+03  1.113008e+01   \nmin    2.023091e+09  5.400000e+01  3.084200e+04  1.000000e+00   \n25%    2.023101e+09  1.150000e+03  4.519800e+04  8.000000e+00   \n50%    2.023111e+09  2.171000e+03  5.241300e+04  1.500000e+01   \n75%    2.023121e+09  3.246000e+03  5.450000e+04  2.200000e+01   \nmax    2.024011e+09  5.258000e+03  5.667300e+04  1.230000e+02   \n\n       absolute_yardline_number  player_weight             x             y  \\\ncount              4.880579e+06   4.880579e+06  4.880579e+06  4.880579e+06   \nmean               6.055045e+01   2.112783e+02  6.050074e+01  2.681190e+01   \nstd                2.305935e+01   2.217747e+01  2.348919e+01  1.000620e+01   \nmin                1.100000e+01   1.530000e+02  4.100000e-01  6.200000e-01   \n25%                4.100000e+01   1.950000e+02  4.263000e+01  1.899000e+01   \n50%                6.100000e+01   2.070000e+02  6.041000e+01  2.685000e+01   \n75%                8.000000e+01   2.250000e+02  7.823000e+01  3.462000e+01   \nmax                1.090000e+02   3.580000e+02  1.198600e+02  5.288000e+01   \n\n                  s             a           dir             o  \\\ncount  4.880579e+06  4.880579e+06  4.880579e+06  4.880579e+06   \nmean   3.019878e+00  2.118335e+00  1.804972e+02  1.815366e+02   \nstd    2.227939e+00  1.415794e+00  1.007162e+02  9.800912e+01   \nmin    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n25%    1.090000e+00  1.010000e+00  9.091000e+01  9.174000e+01   \n50%    2.720000e+00  1.920000e+00  1.795600e+02  1.801400e+02   \n75%    4.620000e+00  3.040000e+00  2.708300e+02  2.715800e+02   \nmax    1.253000e+01  1.712000e+01  3.600000e+02  3.600000e+02   \n\n       num_frames_output   ball_land_x   ball_land_y  \ncount       4.880579e+06  4.880579e+06  4.880579e+06  \nmean        1.164147e+01  6.051581e+01  2.663766e+01  \nstd         5.331537e+00  2.529643e+01  1.543814e+01  \nmin         5.000000e+00 -5.260000e+00 -3.910000e+00  \n25%         8.000000e+00  4.261000e+01  1.330000e+01  \n50%         1.000000e+01  6.051000e+01  2.647000e+01  \n75%         1.400000e+01  7.847000e+01  3.987000e+01  \nmax         9.400000e+01  1.258500e+02  5.733000e+01  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>game_id</th>\n      <th>play_id</th>\n      <th>nfl_id</th>\n      <th>frame_id</th>\n      <th>absolute_yardline_number</th>\n      <th>player_weight</th>\n      <th>x</th>\n      <th>y</th>\n      <th>s</th>\n      <th>a</th>\n      <th>dir</th>\n      <th>o</th>\n      <th>num_frames_output</th>\n      <th>ball_land_x</th>\n      <th>ball_land_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n      <td>4.880579e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.023155e+09</td>\n      <td>2.196409e+03</td>\n      <td>4.955890e+04</td>\n      <td>1.613179e+01</td>\n      <td>6.055045e+01</td>\n      <td>2.112783e+02</td>\n      <td>6.050074e+01</td>\n      <td>2.681190e+01</td>\n      <td>3.019878e+00</td>\n      <td>2.118335e+00</td>\n      <td>1.804972e+02</td>\n      <td>1.815366e+02</td>\n      <td>1.164147e+01</td>\n      <td>6.051581e+01</td>\n      <td>2.663766e+01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.011405e+05</td>\n      <td>1.246426e+03</td>\n      <td>5.210338e+03</td>\n      <td>1.113008e+01</td>\n      <td>2.305935e+01</td>\n      <td>2.217747e+01</td>\n      <td>2.348919e+01</td>\n      <td>1.000620e+01</td>\n      <td>2.227939e+00</td>\n      <td>1.415794e+00</td>\n      <td>1.007162e+02</td>\n      <td>9.800912e+01</td>\n      <td>5.331537e+00</td>\n      <td>2.529643e+01</td>\n      <td>1.543814e+01</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2.023091e+09</td>\n      <td>5.400000e+01</td>\n      <td>3.084200e+04</td>\n      <td>1.000000e+00</td>\n      <td>1.100000e+01</td>\n      <td>1.530000e+02</td>\n      <td>4.100000e-01</td>\n      <td>6.200000e-01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>5.000000e+00</td>\n      <td>-5.260000e+00</td>\n      <td>-3.910000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.023101e+09</td>\n      <td>1.150000e+03</td>\n      <td>4.519800e+04</td>\n      <td>8.000000e+00</td>\n      <td>4.100000e+01</td>\n      <td>1.950000e+02</td>\n      <td>4.263000e+01</td>\n      <td>1.899000e+01</td>\n      <td>1.090000e+00</td>\n      <td>1.010000e+00</td>\n      <td>9.091000e+01</td>\n      <td>9.174000e+01</td>\n      <td>8.000000e+00</td>\n      <td>4.261000e+01</td>\n      <td>1.330000e+01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.023111e+09</td>\n      <td>2.171000e+03</td>\n      <td>5.241300e+04</td>\n      <td>1.500000e+01</td>\n      <td>6.100000e+01</td>\n      <td>2.070000e+02</td>\n      <td>6.041000e+01</td>\n      <td>2.685000e+01</td>\n      <td>2.720000e+00</td>\n      <td>1.920000e+00</td>\n      <td>1.795600e+02</td>\n      <td>1.801400e+02</td>\n      <td>1.000000e+01</td>\n      <td>6.051000e+01</td>\n      <td>2.647000e+01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.023121e+09</td>\n      <td>3.246000e+03</td>\n      <td>5.450000e+04</td>\n      <td>2.200000e+01</td>\n      <td>8.000000e+01</td>\n      <td>2.250000e+02</td>\n      <td>7.823000e+01</td>\n      <td>3.462000e+01</td>\n      <td>4.620000e+00</td>\n      <td>3.040000e+00</td>\n      <td>2.708300e+02</td>\n      <td>2.715800e+02</td>\n      <td>1.400000e+01</td>\n      <td>7.847000e+01</td>\n      <td>3.987000e+01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.024011e+09</td>\n      <td>5.258000e+03</td>\n      <td>5.667300e+04</td>\n      <td>1.230000e+02</td>\n      <td>1.090000e+02</td>\n      <td>3.580000e+02</td>\n      <td>1.198600e+02</td>\n      <td>5.288000e+01</td>\n      <td>1.253000e+01</td>\n      <td>1.712000e+01</td>\n      <td>3.600000e+02</td>\n      <td>3.600000e+02</td>\n      <td>9.400000e+01</td>\n      <td>1.258500e+02</td>\n      <td>5.733000e+01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"#valores nulos\ntrain_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T03:13:25.714842Z","iopub.execute_input":"2025-10-15T03:13:25.715252Z","iopub.status.idle":"2025-10-15T03:13:27.58621Z","shell.execute_reply.started":"2025-10-15T03:13:25.715227Z","shell.execute_reply":"2025-10-15T03:13:27.585302Z"}},"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"game_id                     0\nplay_id                     0\nplayer_to_predict           0\nnfl_id                      0\nframe_id                    0\nplay_direction              0\nabsolute_yardline_number    0\nplayer_name                 0\nplayer_height               0\nplayer_weight               0\nplayer_birth_date           0\nplayer_position             0\nplayer_side                 0\nplayer_role                 0\nx                           0\ny                           0\ns                           0\na                           0\ndir                         0\no                           0\nnum_frames_output           0\nball_land_x                 0\nball_land_y                 0\ndtype: int64"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T03:13:27.587268Z","iopub.execute_input":"2025-10-15T03:13:27.587596Z","iopub.status.idle":"2025-10-15T03:13:27.609481Z","shell.execute_reply.started":"2025-10-15T03:13:27.587568Z","shell.execute_reply":"2025-10-15T03:13:27.608389Z"}},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"      game_id  play_id  player_to_predict  nfl_id  frame_id play_direction  \\\n0  2023090700      101              False   54527         1          right   \n1  2023090700      101              False   54527         2          right   \n2  2023090700      101              False   54527         3          right   \n3  2023090700      101              False   54527         4          right   \n4  2023090700      101              False   54527         5          right   \n\n   absolute_yardline_number player_name player_height  player_weight  ...  \\\n0                        42  Bryan Cook           6-1            210  ...   \n1                        42  Bryan Cook           6-1            210  ...   \n2                        42  Bryan Cook           6-1            210  ...   \n3                        42  Bryan Cook           6-1            210  ...   \n4                        42  Bryan Cook           6-1            210  ...   \n\n          player_role      x      y     s     a     dir       o  \\\n0  Defensive Coverage  52.33  36.94  0.09  0.39  322.40  238.24   \n1  Defensive Coverage  52.33  36.94  0.04  0.61  200.89  236.05   \n2  Defensive Coverage  52.33  36.93  0.12  0.73  147.55  240.60   \n3  Defensive Coverage  52.35  36.92  0.23  0.81  131.40  244.25   \n4  Defensive Coverage  52.37  36.90  0.35  0.82  123.26  244.25   \n\n   num_frames_output  ball_land_x  ball_land_y  \n0                 21    63.259998        -0.22  \n1                 21    63.259998        -0.22  \n2                 21    63.259998        -0.22  \n3                 21    63.259998        -0.22  \n4                 21    63.259998        -0.22  \n\n[5 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>game_id</th>\n      <th>play_id</th>\n      <th>player_to_predict</th>\n      <th>nfl_id</th>\n      <th>frame_id</th>\n      <th>play_direction</th>\n      <th>absolute_yardline_number</th>\n      <th>player_name</th>\n      <th>player_height</th>\n      <th>player_weight</th>\n      <th>...</th>\n      <th>player_role</th>\n      <th>x</th>\n      <th>y</th>\n      <th>s</th>\n      <th>a</th>\n      <th>dir</th>\n      <th>o</th>\n      <th>num_frames_output</th>\n      <th>ball_land_x</th>\n      <th>ball_land_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023090700</td>\n      <td>101</td>\n      <td>False</td>\n      <td>54527</td>\n      <td>1</td>\n      <td>right</td>\n      <td>42</td>\n      <td>Bryan Cook</td>\n      <td>6-1</td>\n      <td>210</td>\n      <td>...</td>\n      <td>Defensive Coverage</td>\n      <td>52.33</td>\n      <td>36.94</td>\n      <td>0.09</td>\n      <td>0.39</td>\n      <td>322.40</td>\n      <td>238.24</td>\n      <td>21</td>\n      <td>63.259998</td>\n      <td>-0.22</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023090700</td>\n      <td>101</td>\n      <td>False</td>\n      <td>54527</td>\n      <td>2</td>\n      <td>right</td>\n      <td>42</td>\n      <td>Bryan Cook</td>\n      <td>6-1</td>\n      <td>210</td>\n      <td>...</td>\n      <td>Defensive Coverage</td>\n      <td>52.33</td>\n      <td>36.94</td>\n      <td>0.04</td>\n      <td>0.61</td>\n      <td>200.89</td>\n      <td>236.05</td>\n      <td>21</td>\n      <td>63.259998</td>\n      <td>-0.22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023090700</td>\n      <td>101</td>\n      <td>False</td>\n      <td>54527</td>\n      <td>3</td>\n      <td>right</td>\n      <td>42</td>\n      <td>Bryan Cook</td>\n      <td>6-1</td>\n      <td>210</td>\n      <td>...</td>\n      <td>Defensive Coverage</td>\n      <td>52.33</td>\n      <td>36.93</td>\n      <td>0.12</td>\n      <td>0.73</td>\n      <td>147.55</td>\n      <td>240.60</td>\n      <td>21</td>\n      <td>63.259998</td>\n      <td>-0.22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023090700</td>\n      <td>101</td>\n      <td>False</td>\n      <td>54527</td>\n      <td>4</td>\n      <td>right</td>\n      <td>42</td>\n      <td>Bryan Cook</td>\n      <td>6-1</td>\n      <td>210</td>\n      <td>...</td>\n      <td>Defensive Coverage</td>\n      <td>52.35</td>\n      <td>36.92</td>\n      <td>0.23</td>\n      <td>0.81</td>\n      <td>131.40</td>\n      <td>244.25</td>\n      <td>21</td>\n      <td>63.259998</td>\n      <td>-0.22</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023090700</td>\n      <td>101</td>\n      <td>False</td>\n      <td>54527</td>\n      <td>5</td>\n      <td>right</td>\n      <td>42</td>\n      <td>Bryan Cook</td>\n      <td>6-1</td>\n      <td>210</td>\n      <td>...</td>\n      <td>Defensive Coverage</td>\n      <td>52.37</td>\n      <td>36.90</td>\n      <td>0.35</td>\n      <td>0.82</td>\n      <td>123.26</td>\n      <td>244.25</td>\n      <td>21</td>\n      <td>63.259998</td>\n      <td>-0.22</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 23 columns</p>\n</div>"},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"train_df.to_numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# N√∫mero de valores √∫nicos por columna\ntrain_df.nunique().sort_values(ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T03:13:32.273937Z","iopub.execute_input":"2025-10-15T03:13:32.274158Z","iopub.status.idle":"2025-10-15T03:13:34.593339Z","shell.execute_reply.started":"2025-10-15T03:13:32.274141Z","shell.execute_reply":"2025-10-15T03:13:34.592344Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"o                           36001\ndir                         36001\nx                           11872\nball_land_x                  7491\ny                            5222\nball_land_y                  5030\nplay_id                      4317\nnfl_id                       1384\nplayer_name                  1383\nplayer_birth_date            1132\na                            1105\ns                            1033\ngame_id                       272\nplayer_weight                 151\nframe_id                      123\nabsolute_yardline_number       99\nnum_frames_output              34\nplayer_position                19\nplayer_height                  16\nplayer_role                     4\nplayer_side                     2\nplay_direction                  2\nplayer_to_predict               2\ndtype: int64"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"# tipo de variable (para saber como usarlas en el modelo)\ndata_types_summary = pd.DataFrame({\n    'Tipo': ['Identificador', 'Identificador', 'Identificador', 'Identificador', \n             'Categor√≠a', 'Categor√≠a', 'Categor√≠a', 'Categor√≠a', \n             'Num√©rica', 'Num√©rica', 'Num√©rica', 'Num√©rica', 'Num√©rica', 'Num√©rica', \n             'Num√©rica', 'Num√©rica', 'Num√©rica', 'Num√©rica', 'Num√©rica', 'Num√©rica', \n             'Booleana', 'Objetivo', 'Objetivo'],\n    'Variable': ['game_id', 'play_id', 'nfl_id', 'player_name', \n                 'player_position', 'player_side', 'player_role', 'play_direction',\n                 'x', 'y', 's', 'a', 'dir', 'o', \n                 'player_height', 'player_weight', 'player_birth_date',\n                 'absolute_yardline_number', 'frame_id', 'num_frames_output',\n                 'player_to_predict', 'ball_land_x', 'ball_land_y']\n})\n\ndata_types_summary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exploraci√≥n de las variables num√©ricas\nA continuaci√≥n, se visualizan los histogramas de las variables num√©ricas del conjunto de datos.\nEl an√°lisis permite verificar la coherencia de los valores, la presencia de sesgos y la necesidad\nde posibles transformaciones o normalizaciones antes del modelado.\n\n**Conclusiones:**\n- Las distribuciones son coherentes con el contexto f√≠sico del juego.\n- No se observan outliers extremos.\n- Las variables `dir` y `o` muestran bimodalidad asociada a la direcci√≥n de la jugada.\n","metadata":{}},{"cell_type":"code","source":"train_df.hist(bins=50, figsize=(20,15))\nplt.tight_layout()\n#plt.savefig('results/attribute_histogram_plots.pdf', format='pdf', dpi=300)#para grabar resultados en pdf\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Variables numericas\nnumeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns\nprint(numeric_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizaci√≥n espacial de los jugadores en el campo (Distribuci√≥n por velocidad)\n","metadata":{}},{"cell_type":"code","source":"#funcion para simular el campo (graficar)\ndef draw_field(ax=None):\n    if ax is None:\n        ax = plt.gca()\n    \n    # L√≠mites del campo (en yardas)\n    ax.set_xlim(0, 120)\n    ax.set_ylim(0, 53.3)\n\n    # Fondo verde del campo\n    ax.add_patch(Rectangle((0, 0), 120, 53.3, linewidth=0, facecolor='#3f995b', zorder=0))\n\n    # L√≠neas principales cada 10 yardas\n    for x in range(0, 121, 10):\n        ax.plot([x, x], [0, 53.3], color='white', linewidth=1)\n\n    # L√≠neas de anotaci√≥n (end zones)\n    ax.plot([10, 10], [0, 53.3], color='gold', linewidth=2)\n    ax.plot([110, 110], [0, 53.3], color='gold', linewidth=2)\n\n    # L√≠nea de medio campo\n    ax.plot([60, 60], [0, 53.3], color='white', linewidth=2, linestyle='--')\n\n    # L√≠neas de borde\n    ax.plot([0, 120], [0, 0], color='white', linewidth=3)\n    ax.plot([0, 120], [53.3, 53.3], color='white', linewidth=3)\n\n    # End zones (fondos)\n    ax.add_patch(Rectangle((0, 0), 10, 53.3, linewidth=0, facecolor='#2b5d38', alpha=0.8))\n    ax.add_patch(Rectangle((110, 0), 10, 53.3, linewidth=0, facecolor='#2b5d38', alpha=0.8))\n\n    # Texto\n    ax.text(5, 26.65, 'HOME', color='white', fontsize=10, ha='center', rotation=90)\n    ax.text(115, 26.65, 'AWAY', color='white', fontsize=10, ha='center', rotation=270)\n\n    # Etiquetas y t√≠tulo\n    ax.set_xlabel('X position (yards)')\n    ax.set_ylabel('Y position (yards)')\n    ax.set_title('Distribuci√≥n de jugadores en el campo (coloreado por velocidad)')\n    return ax\n\n\nfig, ax = plt.subplots(figsize=(12, 6))\ndraw_field(ax)\nsubset = train_df.sample(20000)\n\n\nsc = ax.scatter(\n    subset['x'], subset['y'],\n    c=subset['s'], cmap='coolwarm', alpha=0.4, s=15, edgecolor='none'\n)\n\n# Barra de color\ncbar = plt.colorbar(sc, ax=ax)\ncbar.set_label('Velocidad (yardas/s)')\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#id de las jugadas para visualizar cada una\ntrain_df['play_id'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GRAFICAMOS 1 JUGADA/Seleccionar una jugada espec√≠fica\nplay_id = train_df['play_id'].iloc[3679]  \nplay_df = train_df[train_df['play_id'] == play_id]\n\nprint(f\"Mostrando jugada con play_id = {play_id}, n√∫mero de jugadores: {len(play_df)}\")\n\n# Crear gr√°fico\nfig, ax = plt.subplots(figsize=(12, 6))\ndraw_field(ax)\n\n# Dibujar jugadores\nsc = ax.scatter(\n    play_df['x'], play_df['y'],\n    c=play_df['s'], cmap='coolwarm', s=100, alpha=0.8, edgecolor='k'\n)\n\n# Dibujar direcciones como flechas\nangles_rad = np.deg2rad(play_df['dir'])# Convertir direcci√≥n (en grados) a radianes\nu = np.cos(angles_rad) * play_df['s'] / 2   # componente en x (escalado) --> se multiplica la velocidad por el movimiento\nv = np.sin(angles_rad) * play_df['s'] / 2   # componente en y (escalado)\n\nax.quiver(\n    play_df['x'], play_df['y'],\n    u, v, #movimientos en los ejes\n    angles='xy', scale_units='xy', scale=1, color='black', width=0.0025\n)\n\n# Barra de color (velocidad)\ncbar = plt.colorbar(sc, ax=ax)\ncbar.set_label(\"Velocidad (yardas/s)\")\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#boxplot para ver la distribucion de nuestros datos \n\nplt.figure(figsize=(20, 25))\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(len(numeric_cols)//3 + 1, 3, i)\n    sns.boxplot(x=train_df[col], color='skyblue')\n    plt.title(f'Distribuci√≥n de {col}', fontsize=12)\n    plt.xlabel('')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Analisis boxplot\n1. **Identificadores:** game_id, play_id, frame_id\n2. **Outliers:** player_wight,s,a\n3. **Bimodal:** dir,o\n4. **sin problema:** absolute_yardline_number, ball_land_x, ball_land_y","metadata":{}},{"cell_type":"code","source":"train_df['player_weight'].describe()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[['x','y']].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[['s','a']].quantile([0.95, 0.99])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing = train_df.isnull().mean() * 100\nmissing[missing > 0].sort_values(ascending=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sesgo y curtosis/ para identificar distribucio real\nskewness = train_df[[\"x\", \"y\", \"s\", \"a\", \"o\", \"dir\", \"player_weight\"]].skew()\nkurtosis = train_df[[\"x\", \"y\", \"s\", \"a\", \"o\", \"dir\", \"player_weight\"]].kurt()\n\npd.DataFrame({\"Sesgo\": skewness, \"Curtosis\": kurtosis})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Analisis sesgo y curtosis\n* **x:** platicurtica\n* **y:** Simetrica\n* **s:** Cola a la deracha\n* **a:** Cola a la derecha\n* **o:** Distribucion uniforme sobre angulos\n* **dir:** Bimodal\n\n‚ÄúEl an√°lisis exploratorio evidencia que la mayor√≠a de las variables presentan distribuciones sim√©tricas con curtosis negativa, lo cual indica estabilidad y ausencia de valores extremos. Sin embargo, las variables s, a y player_weight muestran sesgo positivo, por lo que podr√≠an beneficiarse de una transformaci√≥n logar√≠tmica para mejorar la normalidad en modelos param√©tricos.‚Äù","metadata":{}},{"cell_type":"markdown","source":"### Creamos una copia de nuestra base de datos\n\ntrabajaremos con esta copia de ahora en adelante ya se le haran cambios y no queremos cambiar nuestra base datos real","metadata":{}},{"cell_type":"code","source":"# Crear una copia del DataFrame original\ndf_clean = train_df.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean.isnull().values.any()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean.isnull().sum()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VALORES NAN\n\ncomo pudimos ver, nuestra base de datos no cuenta con valores nulos","metadata":{}},{"cell_type":"code","source":"# Duplicados\nduplicados = df_clean.duplicated().sum()\nprint(f'Duplicados encontrados: {duplicados}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"z_scores = np.abs(stats.zscore(df_clean.select_dtypes(include=[np.number])))\noutliers = (z_scores > 3).sum()\nprint('Cantidad de outliers por variable:\\n', outliers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outliers_por_variable = (z_scores > 3).sum(axis=0)\nprint(outliers_por_variable)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_clean.dropna(subset=['x','y','s','a','o','dir','player_weight'], inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lista de variables a analizar\nvariables = ['x', 'y', 's', 'a', 'o', 'dir', 'player_weight']\n\n# Crear una figura \nplt.figure(figsize=(20, 18))\n\nfor i, var in enumerate(variables, 1):\n    plt.subplot(4, 2, i)\n    sns.histplot(df_clean[var], bins=40, kde=True, color='skyblue')\n    sesgo = skew(df_clean[var].dropna())\n    curt = kurtosis(df_clean[var].dropna())\n    plt.title(f'{var} | Sesgo: {sesgo:.3f} | Curtosis: {curt:.3f}', fontsize=12)\n    plt.xlabel(var)\n    plt.ylabel('Frecuencia')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Transformacion de radianes a Sen y Cos\n\nesto se hace para evitar errores de similitud entre radianes y gran diferencia numerica, como por ejemplo el 0 y 360 que estarian en la misma posicion pero numericamente muy alejados","metadata":{}},{"cell_type":"code","source":"#transformacion de radianes a sen y cos\n\n#dir\ndf_clean['dir_rad'] = np.deg2rad(df_clean['dir'])\ndf_clean['dir_sin'] = np.sin(df_clean['dir_rad'])\ndf_clean['dir_cos'] = np.cos(df_clean['dir_rad'])\n\n#o\ndf_clean['o_rad'] = np.deg2rad(df_clean['o'])\ndf_clean['o_sin'] = np.sin(df_clean['o_rad'])\ndf_clean['o_cos'] = np.cos(df_clean['o_rad'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# percentiles para decidir umbral\ndf_clean['s'].quantile([0.90,0.95,0.99])\ndf_clean['a'].quantile([0.90,0.95,0.99])\ndf_clean['player_weight'].quantile([0.90,0.95,0.99])\n\n# transformar/comprimir\ndf_clean['s_log1p'] = np.log1p(df_clean['s'])\ndf_clean['a_log1p'] = np.log1p(df_clean['a'])\ndf_clean['player_weight_log'] = np.log1p(df_clean['player_weight'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:28:49.031411Z","iopub.execute_input":"2025-10-15T01:28:49.031792Z","iopub.status.idle":"2025-10-15T01:28:49.428422Z","shell.execute_reply.started":"2025-10-15T01:28:49.03173Z","shell.execute_reply":"2025-10-15T01:28:49.427319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Normalizacion espacial\n\nnormalizaremos las jugadas en un solo sentido para reducir ruido y complejidad ","metadata":{}},{"cell_type":"code","source":"mask = df_clean['play_direction'] == 'left'\ntrain_df.loc[mask, 'x'] = 120 - train_df.loc[mask, 'x']\ntrain_df.loc[mask, 'y'] = 53.3 - train_df.loc[mask, 'y']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:28:50.905926Z","iopub.execute_input":"2025-10-15T01:28:50.906886Z","iopub.status.idle":"2025-10-15T01:28:51.449103Z","shell.execute_reply.started":"2025-10-15T01:28:50.906855Z","shell.execute_reply":"2025-10-15T01:28:51.447797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#revisar cantidad de categorias\ncat_vars = ['player_position', 'player_side', 'player_role', 'play_direction']\n\nfor var in cat_vars:\n    print(f\"{var}: {df_clean[var].nunique()} categor√≠as\")\n    print(df_clean[var].unique(), \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:28:51.871402Z","iopub.execute_input":"2025-10-15T01:28:51.871712Z","iopub.status.idle":"2025-10-15T01:28:53.95763Z","shell.execute_reply.started":"2025-10-15T01:28:51.871688Z","shell.execute_reply":"2025-10-15T01:28:53.956232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### lista de columnas antes de codificar","metadata":{}},{"cell_type":"code","source":"print(df_clean.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:28:57.038725Z","iopub.execute_input":"2025-10-15T01:28:57.039167Z","iopub.status.idle":"2025-10-15T01:28:57.044392Z","shell.execute_reply.started":"2025-10-15T01:28:57.039141Z","shell.execute_reply":"2025-10-15T01:28:57.043267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Label Encoding (para variables binarias)\nlabel_vars = ['player_side', 'play_direction']\nfor col in label_vars:\n    le = LabelEncoder()\n    df_clean[col] = le.fit_transform(df_clean[col])\n\n# Verificaci√≥n del resultado\nprint(train_df.shape)\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:28:57.441319Z","iopub.execute_input":"2025-10-15T01:28:57.441613Z","iopub.status.idle":"2025-10-15T01:28:59.325603Z","shell.execute_reply.started":"2025-10-15T01:28:57.441592Z","shell.execute_reply":"2025-10-15T01:28:59.324636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"onehot_vars = ['player_position', 'player_role']\n\n# Aplicar One-Hot Encoding solo si a√∫n no est√° hecho\nif not any(col.startswith('player_position_') for col in df_clean.columns):\n    df_clean = pd.get_dummies(df_clean, columns=onehot_vars, drop_first=False)\n\nprint(\"Shape final despu√©s del One-Hot Encoding:\", df_clean.shape)\n\n# Verificaci√≥n r√°pida\ndf_clean.filter(regex='player_position_|player_role_').head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:28:59.873596Z","iopub.execute_input":"2025-10-15T01:28:59.873999Z","iopub.status.idle":"2025-10-15T01:29:03.704441Z","shell.execute_reply.started":"2025-10-15T01:28:59.873974Z","shell.execute_reply":"2025-10-15T01:29:03.70353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### lista de columnas despues de codificar","metadata":{}},{"cell_type":"code","source":"print(df_clean.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:29:03.71237Z","iopub.execute_input":"2025-10-15T01:29:03.712684Z","iopub.status.idle":"2025-10-15T01:29:03.718151Z","shell.execute_reply.started":"2025-10-15T01:29:03.71266Z","shell.execute_reply":"2025-10-15T01:29:03.716992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convertir fecha de nacimiento a tipo datetime\ndf_clean['player_birth_date'] = pd.to_datetime(df_clean['player_birth_date'], errors='coerce') #coerce forza a los valores no validos\n\n# Convertir altura de formato '6-2' (pies-pulgadas) a metros (versi√≥n vectorizada)\ndf_clean[['feet', 'inches']] = df_clean['player_height'].str.extract(r'(\\d+)-(\\d+)').astype(float)\ndf_clean['player_height_m'] = df_clean['feet'] * 0.3048 + df_clean['inches'] * 0.0254\ndf_clean.drop(columns=['feet', 'inches'], inplace=True)\n\n# Edad del jugador (asumimos a√±o 2025)\ndf_clean['player_age'] = 2025 - df_clean['player_birth_date'].dt.year # calculamos la edad \n\n# √çndice de masa corporal (BMI)\ndf_clean['BMI'] = df_clean['player_weight'] / (df_clean['player_height_m'] ** 2)\n\n# Componentes de velocidad (en yardas/s)\ndf_clean['velocity_x'] = df_clean['s'] * np.cos(np.deg2rad(df_clean['dir']))\ndf_clean['velocity_y'] = df_clean['s'] * np.sin(np.deg2rad(df_clean['dir']))\n\n# Magnitud de aceleraci√≥n\ndf_clean['acceleration_abs'] = df_clean['a'].abs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:29:04.501697Z","iopub.execute_input":"2025-10-15T01:29:04.502008Z","iopub.status.idle":"2025-10-15T01:29:17.440863Z","shell.execute_reply.started":"2025-10-15T01:29:04.501988Z","shell.execute_reply":"2025-10-15T01:29:17.44001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Selecciona solo las columnas num√©ricas originales\nnumeric_vars = ['absolute_yardline_number', 'player_weight', 'x', 'y', 's', 'a', 'dir', 'o',\n                'num_frames_output', 'ball_land_x', 'ball_land_y']\n\n# estandarizamos nuestros datos\nscaler = StandardScaler()\ndf_clean[numeric_vars] = scaler.fit_transform(df_clean[numeric_vars])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:29:17.44209Z","iopub.execute_input":"2025-10-15T01:29:17.442319Z","iopub.status.idle":"2025-10-15T01:29:18.893999Z","shell.execute_reply.started":"2025-10-15T01:29:17.442301Z","shell.execute_reply":"2025-10-15T01:29:18.892909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Seleccionar solo columnas num√©ricas\ndf_num = df_clean.select_dtypes(include=['int64', 'float64'])\n\n# Calcular la matriz de correlaci√≥n\ncorr_matrix = df_num.corr()\n\n# Variables objetivo\ntargets = ['x', 'y']\n\n# Mostrar correlaci√≥n con cada objetivo\nfor target in targets:\n    if target in corr_matrix.columns:\n        print(f\"\\nüîπ Correlaciones con {target}:\")\n        display(corr_matrix[target].sort_values(ascending=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:47:24.154082Z","iopub.execute_input":"2025-10-15T01:47:24.154767Z","iopub.status.idle":"2025-10-15T01:47:39.893607Z","shell.execute_reply.started":"2025-10-15T01:47:24.154719Z","shell.execute_reply":"2025-10-15T01:47:39.892935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(corr_matrix[['x', 'y']].sort_values(by='x', ascending=False),\n            annot=True, cmap='coolwarm', center=0)\nplt.title('Correlaci√≥n de variables con x e y')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:46:27.316629Z","iopub.execute_input":"2025-10-15T01:46:27.317001Z","iopub.status.idle":"2025-10-15T01:46:27.850987Z","shell.execute_reply.started":"2025-10-15T01:46:27.316976Z","shell.execute_reply":"2025-10-15T01:46:27.849244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets = ['x', 'y']\ncorr_mean = corr_matrix[targets].abs().mean(axis=1).sort_values(ascending=False)\n\nprint(\"üî∏ Promedio de correlaci√≥n absoluta con ambas salidas:\")\ndisplay(corr_mean)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:29:35.003609Z","iopub.execute_input":"2025-10-15T01:29:35.003986Z","iopub.status.idle":"2025-10-15T01:29:35.015098Z","shell.execute_reply.started":"2025-10-15T01:29:35.003956Z","shell.execute_reply":"2025-10-15T01:29:35.014175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictors = [\n    'ball_land_x', 'ball_land_y', 'absolute_yardline_number',\n    'velocity_x', 'velocity_y',\n    'dir_cos', 'dir_sin', 'o_cos', 'o_sin',\n    's', 'a', 'acceleration_abs',\n    'BMI', 'player_height_m', 'player_weight_log',\n    'num_frames_output'\n]\n\ntargets = ['x', 'y']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T01:29:35.016281Z","iopub.execute_input":"2025-10-15T01:29:35.0166Z","iopub.status.idle":"2025-10-15T01:29:35.035439Z","shell.execute_reply.started":"2025-10-15T01:29:35.016572Z","shell.execute_reply":"2025-10-15T01:29:35.034313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective_lgb(trial, X, y):\n    dtrain = lgb.Dataset(X, label=y)\n\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n        'seed': 42\n    }\n\n    callbacks = [lgb.early_stopping(stopping_rounds=50, verbose=False)]\n\n    # Entrenamiento con validaci√≥n cruzada\n    cv_results = lgb.cv(\n        params=params,\n        train_set=dtrain,\n        folds=KFold(n_splits=5, shuffle=True, random_state=42),\n        num_boost_round=500,\n        callbacks=callbacks\n    )\n\n    #Buscamos la clave correcta para RMSE\n    rmse_key = [k for k in cv_results.keys() if 'rmse-mean' in k]\n    if len(rmse_key) == 0:\n        raise KeyError(f\"No se encontr√≥ la m√©trica RMSE en: {list(cv_results.keys())}\")\n\n    return np.min(cv_results[rmse_key[0]])\n\n\n# ==============================\n#Funci√≥n para ajustar y entrenar modelo\n# ==============================\ndef tune_and_train(X_tr, y_tr, X_va, y_va, n_trials=10):\n    study = optuna.create_study(direction='minimize')\n    study.optimize(lambda t: objective_lgb(t, X_tr, y_tr), n_trials=n_trials, show_progress_bar=True)\n\n    best = study.best_params\n    print(\"Mejores hiperpar√°metros:\", best)\n\n    # Entrenar modelo final con los mejores par√°metros\n    model = lgb.LGBMRegressor(**best, objective='regression', metric='rmse', n_estimators=500)\n    model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric='rmse', verbose=False)\n    return model, study\n\n\n\n# Entrenamiento para x_target e y_target\nprint(\"üîπ Entrenando modelo para x_target...\")\nmodel_x, study_x = tune_and_train(X_train, y_train[:, 0], X_val, y_val[:, 0])\n\nprint(\"üîπ Entrenando modelo para y_target...\")\nmodel_y, study_y = tune_and_train(X_train, y_train[:, 1], X_val, y_val[:, 1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T02:31:21.719545Z","iopub.execute_input":"2025-10-15T02:31:21.719908Z"}},"outputs":[],"execution_count":null}]}